#load relevant packages
library(ggplot2)
library(plyr)
library(dplyr)
library(reshape2)
library(tidyr)
library(zyp)
library(dismo)
library(gbm)
library(MASS)
library(data.table)
library(scales)
library(lubridate)
library(Kendall)
library(gridExtra)
library(boot)
library(spatstat)
library(rowr)
library(mvmeta)
library(zoo)
library(imputeTS)
library(mgcv)

#functions
senP<-function(x,y,reps){
  slopes<-zyp.sen(y~x)$slopes
  intercepts<-zyp.sen(y~x)$intercepts
  slope_ps<-vector('numeric')
  intercept_ps<-vector('numeric')
  n<-length(x)
  
  for (i in 1:reps){
  slope_ps[i]<-wilcox.test(sample(slopes,size=n,replace=TRUE))$p.value
  intercept_ps[i]<-wilcox.test(sample(intercepts,size=n,replace=TRUE))$p.value
  }
  
  slope_p<-median(slope_ps)
  intercept_p<-median(intercept_ps)
  result <- list(slope_p=slope_p,intercept_p=intercept_p)
  return(result)
}

#create a folder called DATN which contains the in situ (IS) and remote sening (RS) data

#Read in the in situ (IS) and remote sening (RS) data
DATN1_IS<-fread("D:/DATN/DATA_DATN1_IS_v1.csv",header=TRUE,stringsAsFactors=TRUE,sep=",")
DATN1_RS<-fread("D:/DATN/DATA_AVHRRtemp_DATNlakes_v3.csv",header=TRUE,stringsAsFactors=TRUE,sep=",")

#merge the IS and RS data
DATN<-rbindlist(list(DATN1_RS,DATN1_IS),fill=TRUE)

#Write the data to csv to save work
fwrite(DATN,"D:/DATN/DATA_datn_v21.csv")

#Read in the data
DATN<-fread("D:/DATN/DATA_datn_v21.csv",header=TRUE,stringsAsFactors=TRUE,sep=",")
DATN$date<-as_date(DATN$date)
DATN$decyear<-decimal_date(DATN$date)

#creat a new subfolder called "DATN_RawByLake"
setwd("D:/DATN/DATN_RawBylake")
setDT(DATN)

#Save a separate file for each lake
DATN[, fwrite(copy(.SD)[, lake := lake], paste0("DATN_", lake,".csv")), by = lake]

#break the lakes into lake with (_big) and without (_lil) remote sensing data avialble
LakeNames<-(unique(DATN$lake))
LakeNames_big<-as.data.table(droplevels(unique(DATN[which(is.na(DATN$mod)==FALSE),]$lake)))
LakeNames_lil<-as.data.table(droplevels(unique(DATN[which(is.na(DATN$mod)==TRUE),]$lake)))

#delete lake from _lil that are also in big
LakeNames_lil<-droplevels(LakeNames_lil[which(LakeNames_lil$V1%in%LakeNames_big$V1==FALSE),])
LakeNames_big<-LakeNames_big$V1
LakeNames_lil<-LakeNames_lil$V1

fwrite(as.data.table(LakeNames_lil),"D:/DATN/LakeNames_lil_v1.csv")
fwrite(as.data.table(LakeNames_big),"D:/DATN/LakeNames_big_v1.csv")

#Prime the loops for interpoalting the data to the same depth and temporal repsolution
rm(list=setdiff(ls(),"senP"))
LakeNames_lil<-as.factor(fread("D:/DATN/LakeNames_lil_v1.csv",header=FALSE)$V1)
hyps<-fread("D:/DATN/DATA_datn_hypsModel_v6.csv",header=TRUE,stringsAsFactors=TRUE,sep=",")
LearningRate<-c(0.8192,0.4096,0.2048,0.1024,0.0512,0.0256,0.0128,0.0064,0.0032,0.0016,0.0008,0.0004,0.0002,0.0001)
depthseq<-fread("D:/DATN/DATA_depthseq_v3.csv")

#run the loops through all the lakes without RS data and compiling interpolated values 
for(lake in LakeNames_lil) {
      tryCatch({
        
        #benchmark
        print(lake)
        
        #read and preprocess the data
        lakedata<-fread(paste0("D:/DATN/DATN_RawBylake/DATN_",lake,".csv",sep=""))
        lakedata$date<-as_date(lakedata$date)
        lakedata$decyear<-decimal_date(lakedata$date)
        lakedata$doy<-yday(lakedata$date)
        lakedata$year<-year(lakedata$date)
        lakedata<-droplevels(lakedata[which(is.na(lakedata$temp)==FALSE),])
        lakedata<-droplevels(lakedata[which(is.na(lakedata$doy)==FALSE),])
        lakedata<-droplevels(lakedata[which(is.na(lakedata$depth)==FALSE),])
        
        
        #Summarise data for use later on
        maxdepths<-lakedata[,.(depth=max(depth)),date]
        maxdepths<-maxdepths[depth>0][depth<quantile(depth,.9)+1]
        lakedata<-lakedata[depth<max(maxdepths$depth)+1]
        doys<-lakedata[,.(mindoy=min(doy),maxdoy=max(doy)),year]
        lakedata<-lakedata[doy>quantile(doys$mindoy,.1)-1&doy<quantile(doys$maxdoy,.9)+1]
        
        #evaluate data coverage and subset to only full temperature profiles
        lakedata<-lakedata[order(date,depth)]
        lakedata[,n:=.N,by=.(date)]
        lakedata[,gap:=c(0,diff(depth,na.rm=TRUE)),by=.(date)]
        lakedata[,':='(meandepth=mean(depth,na.rm=TRUE))]
        lakedata[,mediangap:=median(gap,na.rm=TRUE)/meandepth,by=.(date)]
        #lakedata[,maxgap:=max(gap,na.rm=TRUE)/meandepth,by=.(date)]
        lakedata[,depthcoverage:=(max(depth,na.rm=TRUE)-min(depth,na.rm=TRUE))/meandepth,by=.(date)]
        #lakedata.nointerp<-lakedata[meangap>=0.1|maxgap>=0.2|n<3|is.na(gap)==TRUE]
        
        #special case for certain lakes with less data
        if(lake!="StoraEnvattern"&
           lake!="Stensjon"&
           lake!="Allgjuttern"&
           lake!="Batarino"&
           lake!="Brunnsjon"&
           lake!="Fiolen"&
           lake!="Fracksjon"&
           lake!="OvreSkarsjon"&
           lake!="Remmarsjon"&
           lake!="Rotehogstjarnen"&
           lake!="CrystalBog"&
           lake!="Atitlan"&
           lake!="Nareselka"&
           lake!="StSkarsjon"){
        lakedata<-lakedata[mediangap<1&n>3&depthcoverage>1]}
        
        #interpolate each temperature profile first across depth
        if(nrow(lakedata)>0){
        dates<-unique(lakedata$date)
        depths<-depthseq[depth<max(lakedata$depth,na.rm=TRUE)]$depth
        lakedata_interp<-expand.grid(date=dates,depth=depths)
        lakedata_interp$date<-as_date(lakedata_interp$date)
        lakedata_interp$doy<-yday(lakedata_interp$date)
        lakedata_interp$decyear<-decimal_date(lakedata_interp$date)
        lakedata_interp$year<-year(lakedata_interp$date)
        lakedata_interp$datetime<-NA
        lakedata_interp$lon<-NA
        lakedata_interp$lat<-NA
        lakedata_interp$glwd_id<-NA
        lakedata_interp$mod<-NA
        lakedata_interp$lake<-lake
        lakedata_interp$temp<-NA
        lakedata<-as.data.table(rbind.fill(lakedata,lakedata_interp))
        lakedata<-lakedata[order(date,depth)]
        lakedata<-lakedata[,count:=.N,by=.(date,depth)]
        lakedata<-lakedata[count<2|is.na(temp)=="FALSE"]
        lakedata[,maxtemp:=max(temp,na.rm=TRUE),.(date)]
        lakedata[,mintemp:=min(temp,na.rm=TRUE),.(date)]
        lakedata[depth==0&is.na(temp)==TRUE]$temp<-lakedata[depth==0&is.na(temp)==TRUE]$maxtemp
        lakedata[depth==max(depths)&is.na(temp)==TRUE]$temp<-lakedata[depth==max(depths)&is.na(temp)==TRUE]$mintemp
        lakedata[,temp.interp:=approx(depth,temp,depth)$y,by=.(date)]
        lakedata$temp<-lakedata$temp.interp
        }
        lakedata<-droplevels(lakedata[which(is.na(lakedata$temp)==FALSE),])
        lakedata<-droplevels(lakedata[which(is.na(lakedata$doy)==FALSE),])
        lakedata<-droplevels(lakedata[which(is.na(lakedata$depth)==FALSE),])
        lakedata<-data.table(lakedata[,c(1:12)])
        
        #write the data interpolated across depth
        fwrite(lakedata,paste0("D:/DATN/RefitData_DepthInterp_v1/",lake,".csv",sep=""))
        
        #now interpolate through time 
        #interpolate the residuals to daily timescales
        #set time boundaries
        min_date<-min(lakedata[is.na(temp)==FALSE]$date)
        max_date<-max(lakedata[is.na(temp)==FALSE]$date)
        
        #Set the time series frequency
        temp_time<-seq.Date(from=min_date,to=max_date,by=1)
        temp_time<-as.data.table(temp_time)
        temp_time[,doy:=yday(as_date(temp_time))]
        temp_time<-temp_time[doy<max(lakedata$doy,na.rm=TRUE)&doy>min(lakedata$doy)]
        temp_time[,year:=year(as_date(temp_time))]
        temp_time<-temp_time[year%in%lakedata$year]
        temp_time<-temp_time[,1]
        colnames(temp_time)<-"date"
        
        #interpolate across time separately for each depth
        for (depthvalue in depths){
          tryCatch({
            #print(depthvalue)
          lakedata_depthvalue<-lakedata[depth==depthvalue]
          lakedata_depthvalue<-merge(temp_time,lakedata_depthvalue,all.x=TRUE,by="date")
          lakedata_depthvalue[,doy:=yday(date)]
          lakedata_depthvalue[,decyear:=decimal_date(date)]
          lakedata_depthvalue$depth<-depthvalue
          lakedata_depthvalue[,year:=year(date)]
          
          lakedata_depthvalue<-lakedata_depthvalue[order(-doy)]
          lakedata_depthvalue<-lakedata_depthvalue[cumsum(is.na(temp)==FALSE)!=0]
          lakedata_depthvalue<-lakedata_depthvalue[order(doy)]
          lakedata_depthvalue<-lakedata_depthvalue[cumsum(is.na(temp)==FALSE)!=0]
          
          #special case for Nkugute
          if(lake!="Nkugute"){
          doytempfit<-gam(temp~s(doy),data=lakedata_depthvalue[is.na(temp)==FALSE])
          
          for(i in 1:5){
            tryCatch({
            doytempfit<-gam(temp~s(doy),data=lakedata_depthvalue[is.na(temp)==FALSE],weights=(1/abs(doytempfit$residuals)))
          }, error=function(e){})
            }
          
          doytempresidfit<-gam(abs(doytempfit$residuals)~s(doytempfit$model$doy))
          doytempresidfit.data<-NULL
          doytempresidfit.data$absresiduals<-abs(doytempfit$residuals)
          doytempresidfit.data$doy<-doytempfit$model$doy
            
          for(i in 1:5){tryCatch({
            doytempresidfit<-gam(absresiduals~s(doy),data=doytempresidfit.data,weights=(1/abs(doytempresidfit$residuals)))
          }, error=function(e){})}
          
          }
          
          #again special case for Nkugute
          if(lake=="Nkugute"){
            doytempfit<-lm(temp~(doy),data=lakedata_depthvalue[is.na(temp)==FALSE])
            doytempresidfit.data<-NULL
            doytempresidfit.data$absresiduals<-abs(doytempfit$residuals)
            doytempresidfit.data$doy<-doytempfit$model$doy
            doytempresidfit<-lm(absresiduals~doy,data=doytempresidfit.data)
            }
          
          lakedata_depthvalue$temp_run<-predict(doytempfit,newdata=lakedata_depthvalue)
          lakedata_depthvalue$sd_run<-predict(doytempresidfit,newdata=lakedata_depthvalue)
          lakedata_depthvalue[,':='(temp_z=(temp-temp_run)/sd_run)]
          lakedata_depthvalue<-lakedata_depthvalue[order(date)]
          lakedata_depthvalue$temp_z1<-na.approx(lakedata_depthvalue$temp_z,x=lakedata_depthvalue$date,maxgap=200,na.rm=FALSE)
          
          if(lake!="Nkugute"){
          decyeartempfit<-gam(temp_z~s(decyear),data=lakedata_depthvalue[is.na(temp)==FALSE])}
          
          if(lake=="Nkugute"){
            decyeartempfit<-lm(temp_z~decyear,data=lakedata_depthvalue[is.na(temp)==FALSE])}
          
          
          lakedata_depthvalue$temp_z2<-predict(decyeartempfit,newdata=lakedata_depthvalue)
          lakedata_depthvalue<-lakedata_depthvalue[order(decyear)]
          lakedata_depthvalue[,':='(date_sort1=decyear,date_sort2=decyear)]
          lakedata_depthvalue[is.na(temp)==TRUE]$date_sort1<-NA
          lakedata_depthvalue[is.na(temp)==TRUE]$date_sort2<-NA
          lakedata_depthvalue[,date_sort1:=abs(decyear-na.locf(date_sort1))]
          lakedata_depthvalue<-lakedata_depthvalue[order(-decyear)]
          lakedata_depthvalue[,date_sort2:=abs(decyear-na.locf(date_sort2))]
          lakedata_depthvalue[,interp.dist:=365.25*min(date_sort1,date_sort2),by=seq_len(nrow(lakedata_depthvalue))]
          lakedata_depthvalue$temp_z<-(lakedata_depthvalue$temp_z1+(sqrt(lakedata_depthvalue$interp.dist)*lakedata_depthvalue$temp_z2))/(1+sqrt(lakedata_depthvalue$interp.dist))
          lakedata_depthvalue[,':='(fit=((temp_z*sd_run)+temp_run))]
          lakedata_depthvalue[fit>max(lakedata_depthvalue$temp,na.rm=TRUE)]$fit<-max(lakedata_depthvalue$temp,na.rm=TRUE)
          lakedata_depthvalue[fit<min(lakedata_depthvalue$temp,na.rm=TRUE)]$fit<-min(lakedata_depthvalue$temp,na.rm=TRUE)
          lakedata_depthvalue[,doy:=yday(date)]
          lakedata_depthvalue<-as.data.table(lakedata_depthvalue)
          
          #Merge the habitat data from different depths for single lake into a single file
          # if the merged dataset doesn't exist, create it
          if (!exists("lakedata_clean")){lakedata_clean <- lakedata_depthvalue}
          # if the merged dataset does exist, append to it
          if (exists("lakedata_clean")){lakedata_clean<-rbindlist(list(lakedata_clean,lakedata_depthvalue),fill=TRUE)}
          }, error=function(e){cat("ERROR :",conditionMessage(e), "\n")})
        }
        
        lakedata_clean$date<-as_date(lakedata_clean$date)
        lakedata_clean$doy<-yday(lakedata_clean$date)
        lakedata_clean$decyear<-decimal_date(lakedata_clean$date)
        lakedata_clean$year<-year(lakedata_clean$date)
        lakedata_clean$lake<-lake
        
        #calcualte the robustness of the seasonal data coverage
        doycheck<-unique(lakedata[,.(doy,date)])
        doydensity<-data.table(cbind(density(doycheck$doy,
                            n=max(doycheck$doy)-min(doycheck$doy)+1,
                            from=min(doycheck$doy),
                            to=max(doycheck$doy),
                            adjust=0.2)[["x"]],
                          density(doycheck$doy,
                            n=max(doycheck$doy)-min(doycheck$doy)+1,
                            from=min(doycheck$doy),
                            to=max(doycheck$doy),
                            adjust=0.2)[["y"]]))
        doydensity[,doy_propmax:=V2/max(V2)]
        names(doydensity)<-c("doy","doy_density","doy_density_propmax")
        lakedata_clean<-merge(lakedata_clean,doydensity[,c(1,3)],all.x=T,by="doy")
        depthcheck<-unique(lakedata[,.(depth,date)])
        depthdensity<-data.table(cbind(density(depthcheck$depth,
                                             n=100*(round(max(depthcheck$depth),digits=2))+1,
                                             from=0,
                                             to=max(depthcheck$depth),
                                             adjust=0.8)[["x"]],
                                     density(depthcheck$depth,
                                             n=100*(round(max(depthcheck$depth),digits=2))+1,
                                             from=0,
                                             to=round(max(depthcheck$depth),digits=2),
                                             adjust=0.8)[["y"]]))
        depthdensity<-depthdensity[round(V1,digits=2)%in%round(lakedata_clean$depth,digits=2)]
        depthdensity[,depth_propmax:=V2/max(V2)]
        depthdensity$V1<-round(depthdensity$V1,digits=2)
        names(depthdensity)<-c("depth","depth_density","depth_density_propmax")
        lakedata_clean$depth<-round(lakedata_clean$depth,digits=2)
        lakedata_clean<-merge(lakedata_clean,depthdensity[,c(1,3)],all.x=T,by="depth")
        
        #calculate weight for the robustness of the data in each year
        yearcheck<-unique(lakedata[,.(year,date)])
        yeardensity<-data.table(cbind(density(yearcheck$year,
                                             n=max(yearcheck$year)-min(yearcheck$year)+1,
                                             from=min(yearcheck$year),
                                             to=max(yearcheck$year),
                                             adjust=0.1)[["x"]],
                                     density(yearcheck$year,
                                             n=max(yearcheck$year)-min(yearcheck$year)+1,
                                             from=min(yearcheck$year),
                                             to=max(yearcheck$year),
                                             adjust=0.1)[["y"]]))
        yeardensity[,year_propmax:=V2/max(V2)]
        names(yeardensity)<-c("year","year_density","year_density_propmax")
        lakedata_clean<-merge(lakedata_clean,yeardensity[,c(1,3)],all.x=T,by="year")
        
        #supply the mult factor for each predict_refit reflecting th volume of water at each depth
        #mult factor for surface area at each depth
        rowlake<-which(hyps$lake==paste0(lake,sep=""))
        maxdepth<-hyps[rowlake]$MaxDepth
        sa<-hyps[rowlake]$SA
        volume<-hyps[rowlake]$MeanDepth*sa
        vd<-volume/((sa*maxdepth)/3)
        Fvd<-((1.7*(vd^(-1)))+2.5-(2.4*vd)+(0.23*(vd^3)))
        lakedata_clean$mf_sa<-sa*((1-(lakedata_clean$depth/max(lakedata_clean$depth)))*
                                    (1+(lakedata_clean$depth/max(lakedata_clean$depth))*sin(sqrt(lakedata_clean$depth/max(lakedata_clean$depth))))^Fvd)
        
        ##calcualte habitat spacetime
        lakedata_clean<-merge(lakedata_clean,depthseq,all.x=TRUE,by="depth")
        lakedata_clean$voldays<-lakedata_clean$depthdays*lakedata_clean$mf_sa #in units of km^3*days
        lakedata_clean<-as.data.table(lakedata_clean)
        
        fwrite(lakedata_clean,paste0("D:/DATN/RefitData_v6/",lake,".csv",sep=""))
        
        rm(lakedata_clean)
        
      }, error=function(e){cat("ERROR :",conditionMessage(e), "\n")})
      }

       
rm(list=setdiff(ls(),"senP"))

#Prime the loops but for big lakes with RS data
LakeNames_big<-as.factor(fread("D:/DATN/LakeNames_big_v1.csv",header=FALSE)$V1)
hyps<-fread("D:/DATN/DATA_datn_hypsModel_v6.csv",header=TRUE,stringsAsFactors=TRUE,sep=",")
LearningRate<-c(0.8192,0.4096,0.2048,0.1024,0.0512,0.0256,0.0128,0.0064,0.0032,0.0016,0.0008,0.0004,0.0002,0.0001)
depthseq<-fread("D:/DATN/DATA_depthseq_v3.csv")

#run the loops through all the lakes with RS data and compiling interpolated values
for(lake in LakeNames_big) {
      tryCatch({
        
        print(lake)
        
        #prep the data
        #rowslake<-which(DATN$lake==paste0(lake,sep=""))
        #lakedata<-DATN[rowslake]
        lakedata<-fread(paste0("D:/DATN/DATN_RawBylake/DATN_",lake,".csv",sep=""))
        lakedata$date<-as_date(lakedata$date)
        lakedata$decyear<-decimal_date(lakedata$date)
        lakedata$doy<-yday(lakedata$date)
        lakedata$year<-year(lakedata$date)
        lakedata<-droplevels(lakedata[which(is.na(lakedata$temp)==FALSE),])
        lakedata<-droplevels(lakedata[which(is.na(lakedata$doy)==FALSE),])
        lakedata<-droplevels(lakedata[which(is.na(lakedata$depth)==FALSE),])
        
        #Clear out data to make the time series more useful
        maxdepths<-lakedata[,.(depth=max(depth)),date]
        maxdepths<-maxdepths[depth>0][depth<quantile(depth,.9)]
        lakedata<-lakedata[depth<max(maxdepths$depth)+1]
        doys<-lakedata[,.(mindoy=min(doy),maxdoy=max(doy)),year]
        lakedata<-lakedata[doy>quantile(doys$mindoy,.1)-1&doy<quantile(doys$maxdoy,.9)+1]
        
        
        if(lakedata[is.na(mod)=="FALSE",.N]>110000){
          #lakedata_Surface<-lakedata[sample(nrow(lakedata[which(lakedata$depth==0),]),1000,replace=FALSE),]
          lakedata_Surface<-as.data.table(unique(lakedata[is.na(mod)=="FALSE"]%>%
            group_by(doy)%>%
            sample_n(1000,replace=TRUE)))
          lakedata<-lakedata[is.na(mod)=="TRUE"]
          lakedata<-rbindlist(list(lakedata,lakedata_Surface))
        }
        
        lakedata_AVHRR<-lakedata[is.na(mod)=="FALSE"]
        lakedata<-lakedata[is.na(mod)=="TRUE"]
        lakedata_tops<-lakedata[,.(depth=-0.001),by=.(date)]
        lakedata<-rbind.fill(lakedata,lakedata_tops)
        lakedata<-data.table(lakedata)
        lakedata<-lakedata[order(date,depth)]
        lakedata[,n:=.N,by=.(date)]
        lakedata[,gap:=c(0,diff(depth,na.rm=TRUE,differences=1)),by=.(date)]
        
        lakedata<-lakedata[is.na(lake)==FALSE]
        lakedata[,':='(meandepth=mean(depth,na.rm=TRUE))]
        
        lakedata[,mediangap:=median(gap,na.rm=TRUE)/meandepth,by=.(date)]
        lakedata[,depthcoverage:=(max(depth,na.rm=TRUE)-min(depth,na.rm=TRUE))/meandepth,by=.(date)]
        lakedata<-lakedata[mediangap<1&n>3&depthcoverage>0.4]
        
        #interpolate first across depth
        if(nrow(lakedata)>0){
          dates<-unique(lakedata$date)
          depths<-depthseq[depth<max(lakedata$depth,na.rm=TRUE)]$depth
          lakedata_interp<-expand.grid(date=dates,depth=depths)
          lakedata_interp$lon<-NA
          lakedata_interp$lat<-NA
          lakedata_interp$date<-as_date(lakedata_interp$date)
          lakedata_interp$doy<-yday(lakedata_interp$date)
          lakedata_interp$decyear<-decimal_date(lakedata_interp$date)
          lakedata_interp$year<-year(lakedata_interp$date)
          lakedata_interp$datetime<-NA
          lakedata_interp$glwd_id<-NA
          lakedata_interp$mod<-NA
          lakedata_interp$lake<-lake
          lakedata_interp$temp<-NA
          lakedata<-as.data.table(rbind.fill(lakedata,lakedata_interp))
          lakedata<-lakedata[order(date,depth)]
          lakedata<-lakedata[,count:=.N,by=.(date,depth)]
          lakedata[,maxtemp:=max(temp,na.rm=TRUE),.(date)]
          lakedata[is.na(temp)==FALSE,mindepth:=min(depth,na.rm=TRUE),.(date)]
          
          #set min and maxs in the profile to maximum values
          lakedata[,mindepth:=mean(mindepth,na.rm=TRUE),.(date)]
          lakedata[,mintemp:=min(temp,na.rm=TRUE),.(date)]
          lakedata[is.na(temp)==FALSE,maxdepth:=max(depth,na.rm=TRUE),.(date)]
          lakedata[,maxdepth:=mean(maxdepth,na.rm=TRUE),.(date)]
          lakedata[,':='(meandepth=mean(depth,na.rm=TRUE))]
          lakedata[depth==0&is.finite(temp)==FALSE&mindepth<(0.2*meandepth)]$temp<-lakedata[depth==0&is.finite(temp)==FALSE&mindepth<(0.2*meandepth)]$maxtemp
          lakedata[depth==max(depths)&is.na(temp)==TRUE&(maxdepth/max(depths))>0.2]$temp<-lakedata[depth==max(depths)&is.na(temp)==TRUE&(maxdepth/max(depths))>0.2]$mintemp
          
          lakedata[,temp.interp:=approx(depth,temp,depth)$y,by=.(date)]
          lakedata$temp<-lakedata$temp.interp
        }
        
        lakedata<-droplevels(lakedata[which(is.na(lakedata$temp)==FALSE),])
        lakedata<-droplevels(lakedata[which(is.na(lakedata$doy)==FALSE),])
        lakedata<-droplevels(lakedata[which(is.na(lakedata$depth)==FALSE),])
        lakedata<-lakedata[,c(1:12)]
        
        fwrite(lakedata,paste0("D:/DATN/RefitData_DepthInterp_v1/",lake,".csv",sep=""))
         
        #interpolate the residuals to daily timescales
        #set time boundaries
        lakedata$date<-as_date(lakedata$date)
        min_date<-min(lakedata[is.na(temp)==FALSE]$date)
        max_date<-max(lakedata[is.na(temp)==FALSE]$date)
        
        #Set the time series frequency
        temp_time<-seq.Date(from=min_date,to=max_date,by=1)
        temp_time<-as.data.table(temp_time)
        temp_time[,doy:=yday(as_date(temp_time))]
        temp_time<-temp_time[doy<max(lakedata$doy,na.rm=TRUE)&doy>min(lakedata$doy)]
        temp_time[,year:=year(as_date(temp_time))]
        temp_time<-temp_time[year%in%lakedata$year]
        temp_time<-temp_time[,1]
        
        colnames(temp_time)<-"date"
        
        for (depthvalue in depths){
          tryCatch({
            
            lakedata_depthvalue<-lakedata[depth==depthvalue]
            lakedata_depthvalue<-merge(temp_time,lakedata_depthvalue,all.x=TRUE,by="date")
            lakedata_depthvalue[,doy:=yday(date)]
            lakedata_depthvalue[,decyear:=decimal_date(date)]
            lakedata_depthvalue$depth<-depthvalue
            lakedata_depthvalue[,year:=year(date)]
            
            lakedata_depthvalue<-lakedata_depthvalue[order(-doy)]
            lakedata_depthvalue<-lakedata_depthvalue[cumsum(is.na(temp)==FALSE)!=0]
            lakedata_depthvalue<-lakedata_depthvalue[order(doy)]
            lakedata_depthvalue<-lakedata_depthvalue[cumsum(is.na(temp)==FALSE)!=0]
            
            doytempfit<-gam(temp~s(doy),data=lakedata_depthvalue[is.na(temp)==FALSE])
            
            for(i in 1:5){
              tryCatch({
                doytempfit<-gam(temp~s(doy),data=lakedata_depthvalue[is.na(temp)==FALSE],weights=(1/abs(doytempfit$residuals)))
              }, error=function(e){})
            }
            
            doytempresidfit<-gam(abs(doytempfit$residuals)~s(doytempfit$model$doy))
            doytempresidfit.data<-NULL
            doytempresidfit.data$absresiduals<-abs(doytempfit$residuals)
            doytempresidfit.data$doy<-doytempfit$model$doy
            
            for(i in 1:5){tryCatch({
              doytempresidfit<-gam(absresiduals~s(doy),data=doytempresidfit.data,weights=(1/abs(doytempresidfit$residuals)))
            }, error=function(e){})}
            
            lakedata_depthvalue$temp_run<-predict.gam(doytempfit,newdata=lakedata_depthvalue)
            lakedata_depthvalue$sd_run<-predict.gam(doytempresidfit,newdata=lakedata_depthvalue)
            
            lakedata_depthvalue[,':='(temp_z=(temp-temp_run)/sd_run)]
            
            lakedata_depthvalue<-lakedata_depthvalue[order(date)]
            
            lakedata_depthvalue$temp_z1<-na.approx(lakedata_depthvalue$temp_z,x=lakedata_depthvalue$date,maxgap=200,na.rm=FALSE)
            decyeartempfit<-gam(temp_z~s(decyear),data=lakedata_depthvalue[is.na(temp)==FALSE])
            lakedata_depthvalue$temp_z2<-predict.gam(decyeartempfit,newdata=lakedata_depthvalue)
            
            lakedata_depthvalue<-lakedata_depthvalue[order(decyear)]
            lakedata_depthvalue[,':='(date_sort1=decyear,date_sort2=decyear)]
            lakedata_depthvalue[is.na(temp)==TRUE]$date_sort1<-NA
            lakedata_depthvalue[is.na(temp)==TRUE]$date_sort2<-NA
            lakedata_depthvalue[,date_sort1:=abs(decyear-na.locf(date_sort1))]
            lakedata_depthvalue<-lakedata_depthvalue[order(-decyear)]
            lakedata_depthvalue[,date_sort2:=abs(decyear-na.locf(date_sort2))]
            lakedata_depthvalue[,interp.dist:=365.25*min(date_sort1,date_sort2),by=seq_len(nrow(lakedata_depthvalue))]
            lakedata_depthvalue$temp_z<-(lakedata_depthvalue$temp_z1+(sqrt(lakedata_depthvalue$interp.dist)*lakedata_depthvalue$temp_z2))/(1+sqrt(lakedata_depthvalue$interp.dist))
            lakedata_depthvalue[,':='(fit=((temp_z*sd_run)+temp_run))]
            lakedata_depthvalue[fit>max(lakedata_depthvalue$temp,na.rm=TRUE)]$fit<-max(lakedata_depthvalue$temp,na.rm=TRUE)
            lakedata_depthvalue[fit<min(lakedata_depthvalue$temp,na.rm=TRUE)]$fit<-min(lakedata_depthvalue$temp,na.rm=TRUE)
            lakedata_depthvalue[,doy:=yday(date)]
            lakedata_depthvalue<-as.data.table(lakedata_depthvalue)
            
            
            # Merge the habitat data from lakes into a single file
            # if the merged dataset doesn't exist, create it
            if (!exists("lakedata_clean")){lakedata_clean <- lakedata_depthvalue}
            # if the merged dataset does exist, append to it
            if (exists("lakedata_clean")){lakedata_clean<-rbindlist(list(lakedata_clean,lakedata_depthvalue),fill=TRUE)}
          }, error=function(e){cat("ERROR :",conditionMessage(e), "\n")})
        }
        
        if (lake=="Erie"|
            lake=="Ontario"|
            lake=="Michigan"|
            lake=="Victoria"|
            lake=="Huron"|
            lake=="Superior") {
        clusters_number<-round(nrow(lakelatlon<-as.data.table(unique(lakedata[,c(1,2)])))/100)
        
        if(clusters_number==0){clusters_number<-1}
        
        lakedata_AVHRR$cluster<-kmeans(lakedata_AVHRR[,c(1,2)],clusters_number)$cluster
        lakedata_AVHRR$date<-as_date(lakedata_AVHRR$date)
        temp_time$date<-as_date(temp_time$date)
        
        for (clust in 1:clusters_number){
          tryCatch({
            lakedata_clustvalue<-lakedata_AVHRR[cluster==clust]
            lakedata_clustvalue<-merge(temp_time,lakedata_clustvalue,all.x=TRUE)
            lakedata_clustvalue[,doy:=yday(date)]
            lakedata_clustvalue[,decyear:=decimal_date(date)]
            lakedata_clustvalue$depth<-clust
            lakedata_clustvalue[,year:=year(date)]
            
            lakedata_clustvalue<-lakedata_clustvalue[order(-doy)]
            lakedata_clustvalue<-lakedata_clustvalue[cumsum(is.na(temp)==FALSE)!=0]
            lakedata_clustvalue<-lakedata_clustvalue[order(doy)]
            lakedata_clustvalue<-lakedata_clustvalue[cumsum(is.na(temp)==FALSE)!=0]
            
            doytempfit<-gam(temp~s(doy),data=lakedata_clustvalue[is.na(temp)==FALSE])
            
            
            for(i in 1:5){
              tryCatch({
                doytempfit<-gam(temp~s(doy),data=lakedata_clustvalue[is.na(temp)==FALSE],weights=(1/abs(doytempfit$residuals)))
              }, error=function(e){})
            }
            
            doytempresidfit<-gam(abs(doytempfit$residuals)~s(doytempfit$model$doy))
            doytempresidfit.data<-NULL
            doytempresidfit.data$absresiduals<-abs(doytempfit$residuals)
            doytempresidfit.data$doy<-doytempfit$model$doy
            
            for(i in 1:5){tryCatch({
              doytempresidfit<-gam(absresiduals~s(doy),data=doytempresidfit.data,weights=(1/abs(doytempresidfit$residuals)))
            }, error=function(e){})}
            
            lakedata_clustvalue$temp_run<-predict.gam(doytempfit,newdata=lakedata_clustvalue)
            lakedata_clustvalue$sd_run<-predict.gam(doytempresidfit,newdata=lakedata_clustvalue)
            
            lakedata_clustvalue[,':='(temp_z=(temp-temp_run)/sd_run)]
            
            lakedata_clustvalue<-lakedata_clustvalue[order(date)]
            
            lakedata_clustvalue$temp_z1<-na.approx(lakedata_clustvalue$temp_z,x=lakedata_clustvalue$date,maxgap=200,na.rm=FALSE)
            decyeartempfit<-gam(temp_z~s(decyear),data=lakedata_clustvalue[is.na(temp)==FALSE])
            lakedata_clustvalue$temp_z2<-predict.gam(decyeartempfit,newdata=lakedata_clustvalue)
            
            lakedata_clustvalue<-lakedata_clustvalue[order(decyear)]
            lakedata_clustvalue[,':='(date_sort1=decyear,date_sort2=decyear)]
            lakedata_clustvalue[is.na(temp)==TRUE]$date_sort1<-NA
            lakedata_clustvalue[is.na(temp)==TRUE]$date_sort2<-NA
            lakedata_clustvalue[,date_sort1:=abs(decyear-na.locf(date_sort1))]
            lakedata_clustvalue<-lakedata_clustvalue[order(-decyear)]
            lakedata_clustvalue[,date_sort2:=abs(decyear-na.locf(date_sort2))]
            lakedata_clustvalue[,interp.dist:=365.25*min(date_sort1,date_sort2),by=seq_len(nrow(lakedata_clustvalue))]
            
            lakedata_clustvalue$temp_z<-(lakedata_clustvalue$temp_z1+(lakedata_clustvalue$interp.dist*lakedata_clustvalue$temp_z2))/(1+lakedata_clustvalue$interp.dist)
            lakedata_clustvalue[,':='(fit=((temp_z*sd_run)+temp_run))]
            lakedata_clustvalue[fit>max(lakedata_clustvalue$temp,na.rm=TRUE)]$fit<-max(lakedata_clustvalue$temp,na.rm=TRUE)
            lakedata_clustvalue[fit<min(lakedata_clustvalue$temp,na.rm=TRUE)]$fit<-min(lakedata_clustvalue$temp,na.rm=TRUE)
            
            lakedata_clustvalue[,doy:=yday(date)]
            lakedata_clustvalue<-as.data.table(lakedata_clustvalue)
            
            # Merge the habitat data from lakes into a single file
            # if the merged dataset doesn't exist, create it
            if (!exists("lakedata_clean")){lakedata_clean <- lakedata_clustvalue}
            # if the merged dataset does exist, append to it
            if (exists("lakedata_clean")){lakedata_clean<-rbindlist(list(lakedata_clean,lakedata_clustvalue),fill=TRUE)}
          }, error=function(e){cat("ERROR :",conditionMessage(e), "\n")})
        }
        
        }
        
        lakedata_clean$date<-as_date(lakedata_clean$date)
        lakedata_clean$doy<-yday(lakedata_clean$date)
        lakedata_clean$decyear<-decimal_date(lakedata_clean$date)
        lakedata_clean$year<-year(lakedata_clean$date)
        lakedata_clean$lake<-lake
        
        #Calculate weights for the robustness of the seasonal coverage
        doycheck<-unique(lakedata[,.(doy,date)])
        doydensity<-data.table(cbind(density(doycheck$doy,
                                             n=max(doycheck$doy)-min(doycheck$doy)+1,
                                             from=min(doycheck$doy),
                                             to=max(doycheck$doy),
                                             adjust=0.2)[["x"]],
                                     density(doycheck$doy,
                                             n=max(doycheck$doy)-min(doycheck$doy)+1,
                                             from=min(doycheck$doy),
                                             to=max(doycheck$doy),
                                             adjust=0.2)[["y"]]))
        doydensity[,doy_propmax:=V2/max(V2)]
        names(doydensity)<-c("doy","doy_density","doy_density_propmax")
        lakedata_clean<-merge(lakedata_clean,doydensity[,c(1,3)],all.x=T,by="doy")
        depthcheck<-unique(lakedata[,.(depth,date)])
        depthdensity<-data.table(cbind(density(depthcheck$depth,
                                               n=100*(round(max(depthcheck$depth),digits=2))+1,
                                               from=0,
                                               to=max(depthcheck$depth),
                                               adjust=1)[["x"]],
                                       (density(depthcheck$depth,
                                               n=100*(round(max(depthcheck$depth),digits=2))+1,
                                               from=0,
                                               to=round(max(depthcheck$depth),digits=2),
                                               adjust=1)[["y"]])^(1/10)))
        depthdensity<-depthdensity[round(V1,digits=2)%in%round(lakedata_clean$depth,digits=2)]
        depthdensity[,depth_propmax:=V2/max(V2)]
        depthdensity$V1<-round(depthdensity$V1,digits=2)
        names(depthdensity)<-c("depth","depth_density","depth_density_propmax")
        lakedata_clean$depth<-round(lakedata_clean$depth,digits=2)
        lakedata_clean<-merge(lakedata_clean,depthdensity[,c(1,3)],all.x=T,by="depth")
        
        #calcualte weights for the robustness of the data from each year
        yearcheck<-unique(lakedata[,.(year,date)])
        yeardensity<-data.table(cbind(density(yearcheck$year,
                                              n=max(yearcheck$year)-min(yearcheck$year)+1,
                                              from=min(yearcheck$year),
                                              to=max(yearcheck$year),
                                              adjust=0.1)[["x"]],
                                      density(yearcheck$year,
                                              n=max(yearcheck$year)-min(yearcheck$year)+1,
                                              from=min(yearcheck$year),
                                              to=max(yearcheck$year),
                                              adjust=0.1)[["y"]]))
        yeardensity[,year_propmax:=V2/max(V2)]
        names(yeardensity)<-c("year","year_density","year_density_propmax")
        lakedata_clean<-merge(lakedata_clean,yeardensity[,c(1,3)],all.x=T,by="year")
        
        #supply the mult factor for each predict_refit reflecting th volume of water at each depth
        #mult factor for surface area at each depth
        rowlake<-which(hyps$lake==paste0(lake,sep=""))
        maxdepth<-hyps[rowlake]$MaxDepth
        sa<-hyps[rowlake]$SA
        volume<-hyps[rowlake]$MeanDepth*sa
        vd<-volume/((sa*maxdepth)/3)
        Fvd<-((1.7*(vd^(-1)))+2.5-(2.4*vd)+(0.23*(vd^3)))
        lakedata_clean$mf_sa<-sa*((1-(lakedata_clean$depth/max(lakedata_clean$depth)))*
                                    (1+(lakedata_clean$depth/max(lakedata_clean$depth))*sin(sqrt(lakedata_clean$depth/max(lakedata_clean$depth))))^Fvd)
        
        if (lake=="Erie"|
            lake=="Ontario"|
            lake=="Michigan"|
            lake=="Huron"|
            lake=="Victoria"|
            lake=="Superior") {
        lakedata_clean[depth==0]$mf_sa<-lakedata_clean[depth==0]$mf_sa/(clusters_number+1)
        }
        
        ##calcualte habitat spacetime
        lakedata_clean<-merge(lakedata_clean,depthseq,by="depth")
        lakedata_clean$voldays<-lakedata_clean$depthdays*lakedata_clean$mf_sa #in units of km^3*days
        lakedata_clean<-as.data.table(lakedata_clean)
        
        #Add the surface data to lakedata clean
        fwrite(lakedata_clean,paste0("D:/DATN/RefitData_v6/",lake,".csv",sep=""))
        
        rm(lakedata_clean)
        
      }, error=function(e){cat("ERROR :",conditionMessage(e), "\n")})
    }


rm(list=setdiff(ls(),"senP"))

#Prime the loops for thermal novelty calcualtions
lakenames_lil<-fread("D:/DATN/LakeNames_lil_v1.csv")
lakenames_big<-fread("D:/DATN/LakeNames_big_v1.csv")
lakenames<-c(lakenames_lil$LakeNames_lil,lakenames_big$LakeNames_big)
depthseq<-fread("D:/DATN/DATA_depthseq_v3.csv")

for (lake in lakenames){
  tryCatch({

#set a benchmark
print(lake)
print(Sys.time())

#get the lake data
lakedata_refit<-fread(paste0("D:/DATN/RefitData_v6/",lake,".csv",sep=""))
lakedata_refit<-unique(lakedata_refit[,c(1,2,3,4,5,6,7,8,9,10,11,12,20,21,25,26,27)])
lakedata_refit<-lakedata_refit[is.finite(fit)==TRUE]

lakedata_refit$date<-as_date(lakedata_refit$date)
lakedata_refit$doy<-yday(lakedata_refit$date)
lakedata_refit$decyear<-decimal_date(lakedata_refit$date)
lakedata_refit$year<-year(lakedata_refit$date)
lakedata_refit$lake<-lake
colnames(lakedata_refit)[colnames(lakedata_refit)=="fit"]<-"predict_refit"

if (lake=="Erie"|
    lake=="Ontario"|
    lake=="Michigan"|
    lake=="Huron"|
    lake=="Victoria"|
    lake=="Superior") {
  lakedata_refit[,depthdays:=NULL]
  lakedata_refit<-merge(lakedata_refit,depthseq)
  lakedata_refit$voldays<-lakedata_refit$depthdays*lakedata_refit$mf_sa #in units of km^3*days
}


lakedata_refit[,sampleprob:=voldays]
lakedata_refit[is.na(sampleprob)==TRUE]$sampleprob<-0
yearnumber<-length(unique(lakedata_refit$year))
years<-(unique(lakedata_refit$year))
years<-years[order(years)]

#prep to calcualte ED
for (i in c(round(quantile(c(1:length(years)),probs=c(0.3,0.5,0.7))))){
  tryCatch({
    
    print(years[i])
    
    for(zmd in c(1.1,0.9,0.7,0.5,0.3,0.1)){
      for(smd in c(1.1,0.9,0.7,0.5,0.3,0.1)){
       repeat{
  
    lakedata_yearA<-lakedata_refit[decyear<years[i]][sample(1:.N, 1000000,replace=TRUE,prob=sampleprob)]
    lakedata_yearB<-lakedata_refit[decyear>=years[i]][sample(1:.N, 1000000,replace=TRUE,prob=sampleprob)]
    colnames(lakedata_yearB)<-paste0(colnames(lakedata_yearB),"X")
    lakedata_exp<-as.data.table(cbind(data.frame(lakedata_yearA),
                                      data.frame(lakedata_yearB)))
    lakedata_exp<-lakedata_exp[is.finite(predict_refit)==TRUE&is.finite(predict_refitX)==TRUE]
    
    lakedata_exp$doydist<-abs(lakedata_exp$doyX-lakedata_exp$doy)
    lakedata_exp[,doydist.crt:=abs(doydist-365)]
    lakedata_exp[doydist>183,doydist:=doydist.crt]
    lakedata_exp$doydist<-lakedata_exp$doydist/183
    lakedata_exp<-lakedata_exp[doydist<(smd)]
    lakedata_exp$depthdist<-abs(lakedata_exp$depthX-lakedata_exp$depth)/max(lakedata_exp$depth)
    lakedata_exp<-lakedata_exp[depthdist<(zmd)]
    
    if (exists("lakedata_exp.c")){lakedata_exp.c<-rbindlist(list(lakedata_exp.c,lakedata_exp),fill=TRUE)}
    if (!exists("lakedata_exp.c")){lakedata_exp.c<-lakedata_exp}
    
    if( nrow(lakedata_exp.c)>1000000){
      lakedata_exp.c<-lakedata_exp.c[sample(1:.N, 1000000,replace=FALSE)] 
      break
    }
    gc()
        }
        
        rm(lakedata_exp.c.null)
        rm(lakedata_exp.null)
        gc()
        
        repeat{
          
          lakedata_yearA<-lakedata_refit[decyear<years[i]][sample(1:.N, 1000000,replace=TRUE,prob=sampleprob)]
          lakedata_yearA2<-lakedata_refit[decyear<years[i]][sample(1:.N, 1000000,replace=TRUE,prob=sampleprob)]
          colnames(lakedata_yearA2)<-paste0(colnames(lakedata_yearA2),"2")
          lakedata_exp.null<-as.data.table(cbind(data.frame(lakedata_yearA),
                                            data.frame(lakedata_yearA2)#,
                                            #data.frame(lakedata_yearB)
                                      ))
          rm(lakedata_yearA)
          rm(lakedata_yearA2)
          lakedata_exp.null<-lakedata_exp.null[is.finite(predict_refit)==TRUE&is.finite(predict_refit2)==TRUE]
          lakedata_exp.null<-data.table(lakedata_exp.null)
          lakedata_exp.null$doydist<-abs(lakedata_exp.null$doy2-lakedata_exp.null$doy)
          lakedata_exp.null[,doydist.crt:=abs(doydist-365)]
          lakedata_exp.null[doydist>183,doydist:=doydist.crt]
          lakedata_exp.null$doydist<-lakedata_exp.null$doydist/183
          lakedata_exp.null<-lakedata_exp.null[doydist<(smd)]
          lakedata_exp.null$depthdist<-abs(lakedata_exp.null$depth2-lakedata_exp.null$depth)/max(lakedata_exp.null$depth)
          lakedata_exp.null<-lakedata_exp.null[depthdist<(zmd)]
          
          if (exists("lakedata_exp.c.null")){lakedata_exp.c.null<-rbindlist(list(lakedata_exp.c.null,lakedata_exp.null),fill=TRUE)}
          if (!exists("lakedata_exp.c.null")){lakedata_exp.c.null<-lakedata_exp.null}
          
          if( nrow(lakedata_exp.c.null)>1000000){
            lakedata_exp.c.null<-lakedata_exp.c.null[sample(1:.N, 1000000,replace=FALSE)] 
            break
          }
          gc()
        }
    
        doydepthgrid<-as.data.table(expand.grid(doy=unique(lakedata_refit$doy),depth=unique(lakedata_refit$depth)))
        doydepthgrid<-doydepthgrid[order(depth,doy)]
        maxdepth<-max(lakedata_refit$depth)
        
        if(nrow(doydepthgrid>8000)){
        doydepthgrid$doy_ds<-round(doydepthgrid$doy,digits=-1)
        doydepthgrid$depth_ds<-rep(unique(lakedata_refit$depth)[seq(1,length(unique(lakedata_refit$depth)),3)],each=length(unique(lakedata_refit$doy))*3)[1:nrow(doydepthgrid)]
        doydepthgrid_ds<-unique(doydepthgrid[,c(3,4)])
        doydepthgrid_ds$sd<-0
        
        #k<-1
        rm(output)
        output<-numeric(nrow(doydepthgrid_ds))
        for (k in 1:nrow(doydepthgrid_ds)){
          ids<-which(lakedata_refit$depth<doydepthgrid_ds[k]$depth+(zmd*maxdepth)&
                       lakedata_refit$depth>doydepthgrid_ds[k]$depth-(zmd*maxdepth)&
                       lakedata_refit$doy<doydepthgrid_ds[k]$doy+(smd*183)&
                       lakedata_refit$doy>doydepthgrid_ds[k]$doy-(smd*183)&
                       lakedata_refit$decyear<years[i])
          output[k]<-(lakedata_refit[ids,sd(predict_refit)])
          gc()
          }
        doydepthgrid_ds$sd<-output
        doydepthgrid<-merge(doydepthgrid,doydepthgrid_ds)
        }
        
        if(nrow(doydepthgrid<8001)&
           nrow(doydepthgrid>4000)){
          doydepthgrid$doy_ds<-round(round(doydepthgrid$doy/5)*5,digits=0)
          doydepthgrid$depth_ds<-rep(unique(lakedata_refit$depth)[seq(1,length(unique(lakedata_refit$depth)),2)],each=length(unique(lakedata_refit$doy))*2)[1:nrow(doydepthgrid)]
          doydepthgrid_ds<-unique(doydepthgrid[,c(3,4)])
          doydepthgrid_ds$sd<-0
          
          #k<-1
          rm(output)
          output<-numeric(nrow(doydepthgrid_ds))
          for (k in 1:nrow(doydepthgrid_ds)){
            ids<-which(lakedata_refit$depth<doydepthgrid_ds[k]$depth+(zmd*maxdepth)&
                         lakedata_refit$depth>doydepthgrid_ds[k]$depth-(zmd*maxdepth)&
                         lakedata_refit$doy<doydepthgrid_ds[k]$doy+(smd*183)&
                         lakedata_refit$doy>doydepthgrid_ds[k]$doy-(smd*183)&
                         lakedata_refit$decyear<years[i])
            output[k]<-(lakedata_refit[ids,sd(predict_refit)])
            gc()
          }
          doydepthgrid_ds$sd<-output
          doydepthgrid<-merge(doydepthgrid,doydepthgrid_ds)
          gc()
        }
        
        if(nrow(doydepthgrid<4001)){
        rm(output)
        output<-numeric(nrow(doydepthgrid))
        doydepthgrid$doy_ds<-round(doydepthgrid$doy,digits=-1)
        doydepthgrid$depth_ds<-rep(unique(lakedata_refit$depth)[seq(1,length(unique(lakedata_refit$depth)),3)],each=length(unique(lakedata_refit$doy))*3)[1:nrow(doydepthgrid)]
        
        for (k in 1:nrow(doydepthgrid)){
          ids<-which(lakedata_refit$depth<doydepthgrid[k]$depth+(zmd*maxdepth)&
                     lakedata_refit$depth>doydepthgrid[k]$depth-(zmd*maxdepth)&
                     lakedata_refit$doy<doydepthgrid[k]$doy+(smd*183)&
                     lakedata_refit$doy>doydepthgrid[k]$doy-(smd*183)&
                     lakedata_refit$decyear<years[i])
          output[k]<-(lakedata_refit[ids,sd(predict_refit)])
          gc()
        }
        doydepthgrid$sd<-output
        gc()
        
        }
        lakedata_exp.full<-as.data.table(merge(lakedata_exp.c,doydepthgrid,by=c("doy","depth")))
        
        #calcualte ED
        lakedata_exp.full<-lakedata_exp.full[,':='(ED=sqrt((predict_refitX-predict_refit)^2/(sd^2)))]
        lakedata_exp.full<-as.data.table(lakedata_exp.full)
        lakedata_exp_sum<-lakedata_exp.full[,.(EDmean=mean(ED),
                                               EDmedian=median(ED))]
        
        lakedata_exp.full.null<-as.data.table(merge(lakedata_exp.c.null,doydepthgrid,by=c("doy","depth")))
        
        #calcualte ED for null
        lakedata_exp.full.null<-lakedata_exp.full.null[,':='(ED=sqrt((predict_refit2-predict_refit)^2/(sd^2)))]
        lakedata_exp.full.null<-as.data.table(lakedata_exp.full.null)
        lakedata_exp.sum.null<-lakedata_exp.full.null[,.(EDmean.null=mean(ED),
                                               EDmedian.null=median(ED))]
        
        lakedata_exp_sum<-cbind(lakedata_exp_sum,lakedata_exp.sum.null)
        
  lakedata_exp_sum$doymaxdist<-smd
  lakedata_exp_sum$depthmaxdist<-zmd      
  lakedata_exp_sum$lake<-lake
  lakedata_exp_sum$i<-i
  lakedata_exp_sum$yearsplit<-years[i]
  
  # Merge the data from years into a single file
  
  # if the merged dataset does exist, append to it
  if (exists("lakedata_exp_data")){lakedata_exp_data<-rbindlist(list(lakedata_exp_data,lakedata_exp_sum),fill=TRUE)}
  # if the merged dataset doesn't exist, create it
  if (!exists("lakedata_exp_data")){lakedata_exp_data<-lakedata_exp_sum}

  
  fwrite(lakedata_exp_data,"D:/DATN/DATN_EDdata_v40backup.csv")
  
  gc()
      }
    }
  
}, error=function(e){cat("ERROR :",conditionMessage(e), "\n")})
}

rm(lakedata_exp)

  lakedata_warm<-lakedata_refit[,
                                .(meantemp=weighted.mean(predict_refit,voldays,na.rm=TRUE)),by=year]
  lakedata_warming<-lakedata_warm[,
                                  .(tempmb_slope=zyp.sen(meantemp~year)$coefficients[2],
                                    tempmb_p=senP(y=meantemp,x=year,reps=25)$slope_p)]
  lakedata_warming$lake<-lake  
  
  # Merge the data from years into a single file
  # if the merged dataset doesn't exist, create it
  if (!exists("lakedata_temptrends")){lakedata_temptrends<-lakedata_warming}
  
  # if the merged dataset does exist, append to it
  if (exists("lakedata_temptrends")){lakedata_temptrends<-rbindlist(list(lakedata_temptrends,lakedata_warming),fill=TRUE)}
  
  lakedata_timeseries<-lakedata_refit[is.finite(predict_refit)==TRUE,.(temp_mean=mean(predict_refit,na.rm=TRUE),#add a check for is.finite
                                            temp_min=min(predict_refit,na.rm=TRUE),
                                            temp_max=max(predict_refit,na.rm=TRUE),
                                            decyear_mean=mean(decyear,na.rm=TRUE),
                                            decyear_min=min(decyear,na.rm=TRUE),
                                            decyear_max=max(decyear,na.rm=TRUE),
                                            doy_mean=mean(doy,na.rm=TRUE),
                                            doy_min=min(doy,na.rm=TRUE),
                                            doy_max=max(doy,na.rm=TRUE),
                                            interp.dist_mean=mean(interp.dist,na.rm=TRUE))]
  lakedata_timeseries$lake<-lake  
  
  # Merge the data from years into a single file
  # if the merged dataset doesn't exist, create it
  if (!exists("lakedata_time")){lakedata_time<-lakedata_timeseries}
  
  # if the merged dataset does exist, append to it
  if (exists("lakedata_time")){lakedata_time<-rbindlist(list(lakedata_time,lakedata_timeseries),fill=TRUE)}
  

fwrite(lakedata_temptrends,"D:/DATN/DATN_temptrends_v40.csv")
fwrite(lakedata_exp_data,"D:/DATN/DATN_EDdata_v40.csv")
fwrite(lakedata_time,"D:/DATN/DATN_EDtime_v40.csv")

gc()

}, error=function(e){cat("ERROR :",conditionMessage(e), "\n")})
}


rm(list=setdiff(ls(),"senP"))

#Compile all the files from all servers from all cores
files<-list.files(path = "D:/DATN", pattern=".csv",full.names = TRUE)
files<-files[grep("DATN_EDtime_v40",files)]
EDtime<-do.call("rbind",lapply(files,FUN=function(files){read.csv(files)}))
EDtime<-unique(EDtime)

files<-list.files(path = "D:/DATN", pattern=".csv",full.names = TRUE)
files<-files[grep("DATN_temptrends_v40",files)]
EDtemp<-do.call("rbind",lapply(files,FUN=function(files){read.csv(files)}))
EDtemp<-unique(EDtemp)

files<-list.files(path = "D:/DATN", pattern=".csv",full.names = TRUE)
files<-files[grep("DATN_EDdata_v40",files)]
EDdata<-do.call("rbind",lapply(files,FUN=function(files){read.csv(files)}))
EDdata<-unique(EDdata)

fwrite(EDdata,"D:/DATN/DATA_EDdata_v45.csv")
fwrite(EDtime,"D:/DATN/DATA_EDtime_v45.csv")
fwrite(EDtemp,"D:/DATN/DATA_EDtemp_v45.csv")


####standardize the ED values by subtracting the null value####
EDdata<-fread("D:/DATN/DATA_EDdata_v45.csv",stringsAsFactors = T)
EDdata<-unique(EDdata)[,":="(EDmean.std=EDmean-EDmean.null,
                             EDmedian.std=EDmedian-EDmedian.null)]
fwrite(EDdata,"D:/DATN/DATN_EDdata.std_v45.csv")
EDdata<-fread("D:/DATN/DATN_EDdata.std_v45.csv",stringsAsFactors = T)

####Calcualte the ED summary by lake####
EDdata.lake<-EDdata[,.(EDmean=mean(abs(EDmean.std)),
                       EDmedian=median(abs(EDmedian.std))
                       ),.(lake)]

lakenames<-unique(EDdata.lake$lake)
EDdata.lake$EDmean.std.p<-1
EDdata.lake$EDmedian.std.p<-1

for (j in 1:length(lakenames)){
  lakename<-lakenames[j]
  EDdata_sub<-EDdata[lake==lakename]
  n<-2*(max(EDdata_sub$yearsplit)-min(EDdata_sub$yearsplit))
  
  mean_ps<-vector('numeric')
  median_ps<-vector('numeric')
  
  for (i in 1:1000){
    mean_ps[i]<-wilcox.test(sample(EDdata_sub$EDmean.std,size=n,replace=TRUE),mu=0)$p.value
    median_ps[i]<-wilcox.test(sample(EDdata_sub$EDmedian.std,size=n,replace=TRUE),mu=0)$p.value
  }
  
  EDdata.lake[j]$EDmean.std.p<-mean(mean_ps)
  EDdata.lake[j]$EDmedian.std.p<-median(median_ps)
}

summary(EDdata.lake)
fwrite(EDdata.lake,"D:/DATN/DATN_EDdata.lake_v45.csv")

#Look for important predicting lake caracteristics from hydrolakes (temperature, warming, and morphometry)
DATN_ED<-fread("D:/DATN/DATN_EDdata.std_v45.csv",header=TRUE,stringsAsFactors = TRUE)
DATN_lakeinfo<-fread("D:/DATN/DATA_datn_HydroLakesInfo_v2.csv",header=TRUE,stringsAsFactors = TRUE) #needs editing
DATN_time<-fread("D:/DATN/DATA_EDtime_v45.csv",header=TRUE,stringsAsFactors = TRUE)
DATN_warm<-fread("D:/DATN/DATA_EDtemp_v45.csv",header=TRUE,stringsAsFactors = TRUE)

DATN_predict<-merge(DATN_ED,DATN_time,all.x=TRUE)
DATN_predict<-merge(DATN_predict,DATN_lakeinfo,all.x=TRUE)
DATN_predict<-unique(DATN_predict)
DATN_warm<-unique(DATN_warm)
DATN_warm<-DATN_warm[,lapply(.SD, mean),.(lake)]
DATN_predict<-merge(DATN_predict,DATN_warm,all.x=TRUE,allow.cartesian=TRUE)
DATN_predict<-unique(DATN_predict)
DATN_predict$abslat<-abs(DATN_predict$lat)
DATN_predict$loghl.Lake_area<-log(DATN_predict$hl.Lake_area)
DATN_predict$loghl.Elevation<-log(DATN_predict$hl.Elevation-min(DATN_predict$hl.Elevation,na.rm=TRUE)+100)
DATN_predict$loghl.Depth_avg<-log(DATN_predict$hl.Depth_avg)
DATN_predict$logMeanDepth<-log(DATN_predict$MeanDepth*1000)
DATN_predict$abstempmb_slope<-abs(DATN_predict$tempmb_slope)
DATN_predict$logEDmean.std<-log(DATN_predict$EDmean.std+1)
DATN_predict$logEDmedian.std<-log(DATN_predict$EDmedian.std+1)

#write the data to save work
fwrite(DATN_predict,"D:/DATN/DATN_predict_v45.csv")

#Prime the loops for the BRT optimization
DATN_predict<-fread("D:/DATN/DATN_predict_v45.csv",header=TRUE,stringsAsFactors = TRUE)
LearningRate<-c(0.8192,0.4096,0.2048,0.1024,0.0512,0.0256,0.0128,0.0064,0.0032,0.0016,0.0008,0.0004,0.0002,0.0001,0.00005,0.00001,0.000005,0.000001,0.0000005,0.0000001,0.00000005,0.00000001)
bf<-0.1

for(lr in LearningRate){
  tryCatch({
    
    BRT<-gbm.step(
      data=DATN_predict[is.na(EDmean.std)==FALSE], #maybe include cutoff for seasonal coverage
      gbm.x = c(
        #1,#lake
        2,#doy limit
        3,#depth limit
        13,#temp mean
        17,#decyear_min
        18,#decyear_max
        20,#doy_min
        21,#doy_max
        54,#log hl.Lake_area
        57,#log mean depth
        44,#residence time
        45,#elevation
        53#abslat
      ),
      gbm.y = 59,
      family = "gaussian", 
      tree.complexity = 10,
      learning.rate = lr,
      max.trees=10000,
      bag.fraction = bf
      )
    
    if(is.null(BRT)==FALSE){
      if(BRT$n.trees>2000) break}
    
  }, error=function(e){cat("ERROR :",conditionMessage(e), "\n")})
}

saveRDS(BRT, "D:/DATN/MODEL_brt_v45.rds")
BRT<-readRDS("D:/DATN/MODEL_brt_v45.rds")

#remove variabiloty attributable to doy coverage and decyear coverage
DATN_predict$predicted_logEDmean.std<-BRT$fit
DATN_predict_std<-DATN_predict
DATN_predict_std<-DATN_predict_std[rep(DATN_predict_std[,.I],100)]
DATN_predict_std$doy_min<-sample(DATN_predict$doy_min,nrow(DATN_predict_std),replace=TRUE)
DATN_predict_std$doy_max<-sample(DATN_predict$doy_max,nrow(DATN_predict_std),replace=TRUE)
DATN_predict_std$decyear_min<-sample(DATN_predict$decyear_min,nrow(DATN_predict_std),replace=TRUE)
DATN_predict_std$decyear_max<-sample(DATN_predict$decyear_max,nrow(DATN_predict_std),replace=TRUE)
DATN_predict_std$predicted_logEDmean.std2<-predict.gbm(BRT,
                                                       newdata=DATN_predict_std,
                                                       n.trees=BRT$gbm.call$best.trees, 
                                                       type="response")
DATN_predict_std<-DATN_predict_std[,.(predicted_logEDmean.std2=mean(predicted_logEDmean.std2)),.(lake,doymaxdist,depthmaxdist,yearsplit)]
DATN_predict<-merge(DATN_predict,DATN_predict_std)
DATN_predict$predicted_EDmean.std2<-exp(DATN_predict$predicted_logEDmean.std2)-1

#Calculate the modeled data for the figure 4 showing ED as a function of 
#lake charactersitics and the bin size
meandepths<-seq(from = min(DATN_predict[is.na(logMeanDepth)==FALSE]$logMeanDepth), 
                to = max(DATN_predict[is.na(logMeanDepth)==FALSE]$logMeanDepth),
                by = (max(DATN_predict[is.na(logMeanDepth)==FALSE]$logMeanDepth)-
                        min(DATN_predict[is.na(logMeanDepth)==FALSE]$logMeanDepth))/1000)
meantemps<-seq(from = min(DATN_predict$temp_mean), 
               to = max(DATN_predict$temp_mean),
               by = (max(DATN_predict$temp_mean)-
                       min(DATN_predict$temp_mean))/1000)
doylimits<-c(0.1,0.3,0.5,0.7,0.9,1.1)
depthlimits<-c(0.1,0.3,0.5,0.7,0.9,1.1)

DATN_predict_new1<-expand.grid(logMeanDepth=meandepths,doymaxdist=doylimits)
DATN_predict_new1$temp_mean<-median(DATN_predict$temp_mean)
DATN_predict_new1$depthmaxdist<-1.1
DATN_predict_new1$type<-1

DATN_predict_new2<-expand.grid(temp_mean=meantemps,doymaxdist=doylimits)
DATN_predict_new2$logMeanDepth<-median(DATN_predict$logMeanDepth)
DATN_predict_new2$depthmaxdist<-1.1
DATN_predict_new2$type<-2

DATN_predict_new3<-expand.grid(logMeanDepth=meandepths,depthmaxdist=depthlimits)
DATN_predict_new3$temp_mean<-median(DATN_predict$temp_mean)
DATN_predict_new3$doymaxdist<-1.1
DATN_predict_new3$type<-3

DATN_predict_new4<-expand.grid(temp_mean=meantemps,depthmaxdist=depthlimits)
DATN_predict_new4$logMeanDepth<-median(DATN_predict$logMeanDepth)
DATN_predict_new4$doymaxdist<-1.1
DATN_predict_new4$type<-4

DATN_predict_new<-as.data.table(rbind.fill(DATN_predict_new1,DATN_predict_new2,DATN_predict_new3,DATN_predict_new4))
DATN_predict_new<-DATN_predict_new[rep(DATN_predict_new[,.I],100)]

DATN_predict_new$doy_min<-sample(DATN_predict$doy_min,nrow(DATN_predict_new),replace=TRUE)
DATN_predict_new$doy_max<-sample(DATN_predict$doy_max,nrow(DATN_predict_new),replace=TRUE)
DATN_predict_new$decyear_min<-sample(DATN_predict$decyear_min,nrow(DATN_predict_new),replace=TRUE)
DATN_predict_new$decyear_max<-sample(DATN_predict$decyear_max,nrow(DATN_predict_new),replace=TRUE)

DATN_predict_new[type==1]$temp_mean<-sample(DATN_predict$temp_mean,nrow(DATN_predict_new[type==1]),replace=TRUE)
DATN_predict_new[type==2]$logMeanDepth<-sample(DATN_predict$logMeanDepth,nrow(DATN_predict_new[type==2]),replace=TRUE)
DATN_predict_new[type==3]$temp_mean<-sample(DATN_predict$temp_mean,nrow(DATN_predict_new[type==3]),replace=TRUE)
DATN_predict_new[type==4]$logMeanDepth<-sample(DATN_predict$logMeanDepth,nrow(DATN_predict_new[type==4]),replace=TRUE)

DATN_predict_new$predicted_logEDmean.std2<-predict.gbm(BRT,
                                                   newdata=DATN_predict_new,
                                                   n.trees=BRT$gbm.call$best.trees, 
                                                   type="response")
DATN_predict_new$predicted_EDmean.std2<-exp(DATN_predict_new$predicted_logEDmean.std2)-1

#write the data to save work
fwrite(DATN_predict,"D:/DATN/DATN_predict.fit_v45.csv")
DATN_predict.fit<-fread("D:/DATN/DATN_predict.fit_v45.csv",header=TRUE,stringsAsFactors = TRUE)

fwrite(DATN_predict_new,"D:/DATN/DATN_predict.new_v45.csv")
DATN_predict_new<-fread("D:/DATN/DATN_predict.new_v45.csv",header=TRUE,stringsAsFactors = TRUE)

DATN_predict_new.means1<-DATN_predict_new[type==1,.(predicted_logEDmean.std2=mean(predicted_logEDmean.std2),
                                            predicted_EDmean.std2=mean(predicted_EDmean.std2)),
                                         .(logMeanDepth,doymaxdist,depthmaxdist,type)]
DATN_predict_new.means2<-DATN_predict_new[type==2,.(predicted_logEDmean.std2=mean(predicted_logEDmean.std2),
                                                    predicted_EDmean.std2=mean(predicted_EDmean.std2)),
                                          .(doymaxdist,temp_mean,depthmaxdist,type)]
DATN_predict_new.means3<-DATN_predict_new[type==3,.(predicted_logEDmean.std2=mean(predicted_logEDmean.std2),
                                                    predicted_EDmean.std2=mean(predicted_EDmean.std2)),
                                          .(logMeanDepth,doymaxdist,depthmaxdist,type)]
DATN_predict_new.means4<-DATN_predict_new[type==4,.(predicted_logEDmean.std2=mean(predicted_logEDmean.std2),
                                                    predicted_EDmean.std2=mean(predicted_EDmean.std2)),
                                          .(doymaxdist,temp_mean,depthmaxdist,type)]
DATN_predict_new.means<-as.data.table(rbind.fill(DATN_predict_new.means1,
                                                 DATN_predict_new.means2,
                                                 DATN_predict_new.means3,
                                                 DATN_predict_new.means4))

fwrite(DATN_predict_new.means,"D:/DATN/DATN_predict_new.means_v45.csv")
DATN_predict_new.means<-fread("D:/DATN/DATN_predict_new.means_v45.csv",header=TRUE,stringsAsFactors = TRUE)

#Calcualte the ED summary by splits averaging over lakes, repeated with standardized data####
EDdata<-fread("D:/DATN/DATN_predict.fit_v45.csv",stringsAsFactors = T)
names(EDdata)
EDdata.splits<-EDdata[,.(EDmean=mean(abs(predicted_EDmean.std2)),
                         EDmedian=median(abs(predicted_EDmean.std2))),.(doymaxdist,depthmaxdist)]
summary(EDdata.splits)
fwrite(EDdata.splits,"D:/DATN/DATN_EDdata.splits.std2_v45.csv")

#smooth the data, repeated with standardized data
doymaxdist<-seq(from = 0.1, to = 1.1, by = 0.01)
depthmaxdist<-seq(from = 0.1, to = 1.1, by = 0.01)
EDdata.splits.interp<-expand.grid(doymaxdist,depthmaxdist)
names(EDdata.splits.interp)<-c("doymaxdist","depthmaxdist")
fit<-gam(data=EDdata.splits,EDmean~s(doymaxdist,depthmaxdist))
EDdata.splits.interp$EDmean<-(predict(fit,EDdata.splits.interp))
summary(EDdata.splits.interp)

fwrite(EDdata.splits.interp,"D:/DATN/DATN_EDdata.splits.interp.std2_v45.csv")

#Calcualte the ED summary by splits separately for each lake, repeated with standardized data####
EDdata.lakesplits<-EDdata[,.(EDmean=mean(abs(predicted_EDmean.std2)),
                             EDmedian=median(abs(predicted_EDmean.std2))),.(doymaxdist,depthmaxdist,lake)]
summary(EDdata.lakesplits)
fwrite(EDdata.lakesplits,"D:/DATN/DATN_EDdata.lakesplits.std2_v45.csv")

#smooth the data sepaartely for each lake, repeated with standardized data
EDdata.lakesplits<-fread("D:/DATN/DATN_EDdata.lakesplits.std2_v45.csv",stringsAsFactors = T)
for(lakename in droplevels(unique(EDdata$lake))){
  doymaxdist<-seq(from = 0.1, to = 1.1, by = 0.01)
  depthmaxdist<-seq(from = 0.1, to = 1.1, by = 0.01)
  EDdata.splits.interp<-expand.grid(doymaxdist,depthmaxdist)
  names(EDdata.splits.interp)<-c("doymaxdist","depthmaxdist")
  fit<-gam(data=EDdata.lakesplits[lake==lakename],EDmean~s(doymaxdist,depthmaxdist))
  EDdata.splits.interp$EDmean<-(predict(fit,EDdata.splits.interp))
  EDdata.splits.interp$lake<-lakename
  # if the merged dataset does exist, append to it
  if (exists("EDdata.lakesplits.interp")){EDdata.lakesplits.interp<-rbindlist(list(EDdata.lakesplits.interp,EDdata.splits.interp),fill=TRUE)}
  # if the merged dataset doesn't exist, create it
  if (!exists("EDdata.lakesplits.interp")){EDdata.lakesplits.interp<-EDdata.splits.interp}
}

fwrite(EDdata.lakesplits.interp,"D:/DATN/DATN_EDdata.lakesplits.interp.std2_v45.csv")

#Lakewide average values####
EDdata.lakesplits.interp<-fread("D:/DATN/DATN_EDdata.lakesplits.interp.std2_v45.csv")
summary(EDdata.lakesplits.interp)

EDdata.lakemedians<-EDdata.lakesplits.interp[,.(EDmedian=median(EDmean)),.(lake)]
summary(EDdata.lakemedians)
fwrite(EDdata.lakemedians,"D:/DATN/DATN_EDdata.lakemedians_v45.csv")

EDdata.lakemeans<-EDdata.lakesplits.interp[,.(EDmean=mean(EDmean)),.(lake)]
summary(EDdata.lakemeans)
fwrite(EDdata.lakemeans,"D:/DATN/DATN_EDdata.lakemeans_v45.csv")

#Merge the lakewide data with the significance values####
EDdata.lakemeans<-fread("D:/DATN/DATN_EDdata.lakemeans_v45.csv",stringsAsFactors = T)
EDdata.lakesig<-fread("D:/DATN/DATN_EDdata.lake_v45.csv",stringsAsFactors = T)
summary(EDdata.lakesig)
EDdata.lakemeans<-merge(EDdata.lakemeans,EDdata.lakesig[,c(1,4,5)])

fwrite(EDdata.lakemeans,"D:/DATN/DATN_EDdata.lakes_v45.csv")
summary(EDdata.lakemeans)

#the percent change when not being able to shift across depth
-100*((mean(EDdata.lakesplits.interp$EDmean)-
  mean(EDdata.lakesplits.interp[depthmaxdist==0.1,.(EDmean=mean(EDmean)),.(lake)]$EDmean))/
  mean(EDdata.lakesplits.interp$EDmean))

#the percent change when not being able to shift across season
-100*((mean(EDdata.lakesplits.interp$EDmean)-
         mean(EDdata.lakesplits.interp[doymaxdist==0.1,.(EDmean=mean(EDmean)),.(lake)]$EDmean))/
        mean(EDdata.lakesplits.interp$EDmean))

#the percent change when not being able to shift across season and depth
-100*((mean(EDdata.lakesplits.interp$EDmean)-
         mean(EDdata.lakesplits.interp[doymaxdist==0.1&depthmaxdist==0.1,.(EDmean=mean(EDmean)),.(lake)]$EDmean))/
        mean(EDdata.lakesplits.interp$EDmean))

#the percent change when being able to shift across depth
-100*((mean(EDdata.lakesplits.interp$EDmean)-
         mean(EDdata.lakesplits.interp[depthmaxdist==1.1,.(EDmean=mean(EDmean)),.(lake)]$EDmean))/
        mean(EDdata.lakesplits.interp$EDmean))

#the percent change when being able to shift across season
-100*((mean(EDdata.lakesplits.interp$EDmean)-
         mean(EDdata.lakesplits.interp[doymaxdist==1.1,.(EDmean=mean(EDmean)),.(lake)]$EDmean))/
        mean(EDdata.lakesplits.interp$EDmean))

#the percent change when being able to shift across season and depth
-100*((mean(EDdata.lakesplits.interp$EDmean)-
         mean(EDdata.lakesplits.interp[doymaxdist==1.1&depthmaxdist==1.1,.(EDmean=mean(EDmean)),.(lake)]$EDmean))/
        mean(EDdata.lakesplits.interp$EDmean))
  

summary(EDdata.lakesplits.interp[doymaxdist==0.1,.(EDmean=mean(EDmean)),.(lake)])
summary(EDdata.lakesplits.interp[doymaxdist==1.1,.(EDmean=mean(EDmean)),.(lake)])
summary(EDdata.lakesplits.interp[depthmaxdist==0.1,.(EDmean=mean(EDmean)),.(lake)])
summary(EDdata.lakesplits.interp[depthmaxdist==1.1,.(EDmean=mean(EDmean)),.(lake)])

#the volume comprised by the lakes in this study
lakeinfo<-fread("DATA_datn_lakeinfo_v3.csv")
summary(lakeinfo)
sum(lakeinfo$Volume)

#total lake surface freshwater volume
181.9-(181.9*.44)

#proportion of freshwater lake volume in this study
(sum(lakeinfo$Volume)/1000)/(181.9-(181.9*.44))

#supplementary information
EDdata.lakemeans<-fread("D:/DATN/DATN_EDdata.lakemeans_v45.csv")
DATN_lakeinfo<-fread("D:/DATN/DATA_datn_HydroLakesInfo_v2.csv",header=TRUE,stringsAsFactors = TRUE) #needs editing
DATN_time<-fread("D:/DATN/DATA_EDtime_v45.csv",header=TRUE,stringsAsFactors = TRUE)
DATN_warm<-fread("D:/DATN/DATA_EDtemp_v45.csv",header=TRUE,stringsAsFactors = TRUE)
EDdata.lakesig<-fread("D:/DATN/DATN_EDdata.lake_v45.csv",stringsAsFactors = T)

DATN_SI<-merge(EDdata.lakemeans,DATN_time,all.x=TRUE)
DATN_SI<-merge(DATN_SI,DATN_lakeinfo,all.x=TRUE)
DATN_SI<-unique(DATN_SI)
DATN_warm<-unique(DATN_warm)
DATN_warm<-DATN_warm[,lapply(.SD, mean),.(lake)]
DATN_SI<-merge(DATN_SI,DATN_warm,all.x=TRUE,allow.cartesian=TRUE)
DATN_SI<-merge(DATN_SI,EDdata.lakesig,all.x=TRUE)

fwrite(DATN_SI,"D:/DATN/DATN_EDdata.lakes_SI_v2.csv")
